# TiDB集群搭建



## 系统机器准备

| 操作系统 | 版本 | IP            | 备注           |
| -------- | ---- | ------------- | -------------- |
| CentosOS | 8.2  | 192.168.1.120 | 中控机/PD/TiKV |
| CentosOS | 8.2  | 192.168.1.145 | PD/TiKV/TiDB   |
| CentosOS | 8.2  | 192.168.1.211 | PD/TiKV/TiDB   |

## 系统服务准备

### ntp服务

由于虚拟机版本8+已不支持ntp软件包，故使用wlnmp来安装ntp服务

```
#添加wlnmp源
rpm -ivh http://mirrors.wlnmp.com/centos/wlnmp-release-centos.noarch.rpm
#安装ntp服务
yum install wntp
#时间同步
ntpdate ntp1.aliyun.com
```

### ssh互信

以root用户登录，并创建tidb用户并设置密码

```
useradd tidb
passwd tidb
```

执行vimudo，配置sudo免密

```
visudo

#进入配置文件在最后添加
tidb ALL=(ALL) NOPASSWD: ALL
```

中控机以tidb登录，生成密钥

```
#执行ssh-keygen，一路回车
ssh-keygen
#进入.ssh目录下,拷贝公钥到服务机器
ssh-copy-id -i ~/.ssh/id_rsa.pub 192.168.1.145
ssh-copy-id -i ~/.ssh/id_rsa.pub 192.168.1.211
#中控机作为服务机器，因此也需要免密
cat id_rsa.pub >> authorized_keys
```

中控机测试免密

```
#ssh连接
ssh 192.168.1.120
ssh 192.168.1.145
ssh 192.168.1.211
#执行
sudo -su root
```

## 安装TiUP 

#### 在中控机上安装 TiUP 组件

```
#安装 TiUP 工具
curl --proto '=https' --tlsv1.2 -sSf https://tiup-mirrors.pingcap.com/install.sh | sh
#重新声明全局环境变量
source .bash_profile
#确认 TiUP 工具是否安装
which tiup
#安装 TiUP cluster 组件
tiup cluster
#如果已经安装，则更新 TiUP cluster 组件至最新版本
tiup update --self && tiup update cluster
#验证当前 TiUP cluster 版本信息
tiup --binary cluster
```

#### 初始化配置文件

```
global:
  user: "tidb"
  ssh_port: 22
  deploy_dir: "/tidb-deploy"
  data_dir: "/tidb-data"
monitored:
  node_exporter_port: 9100
  blackbox_exporter_port: 9115
server_configs:
  tidb:
    log.slow-threshold: 300
    binlog.enable: false
    binlog.ignore-error: false
  tikv:
    readpool.storage.use-unified-pool: false
    readpool.coprocessor.use-unified-pool: true
  pd:
    schedule.leader-schedule-limit: 4
    schedule.region-schedule-limit: 2048
    schedule.replica-schedule-limit: 64 
pd_servers:
  - host: 192.168.1.120
  - host: 192.168.1.145
  - host: 192.168.1.211
tidb_servers:
  - host: 192.168.1.145
  - host: 192.168.1.211
tikv_servers:
  - host: 192.168.1.120
  - host: 192.168.1.145
  - host: 192.168.1.211
monitoring_servers:
  - host: 192.168.1.120
grafana_servers:
  - host: 192.168.1.120
alertmanager_servers:
  - host: 192.168.1.120
```

#### 集群部署

```
#执行部署命令
tiup cluster deploy tidb-test v4.0.0 ./topology.yaml -y
#查看 TiUP 管理的集群情况
tiup cluster list
#检查部署的 TiDB 集群情况
tiup cluster display tidb-test
#启动集群
tiup cluster start tidb-test
```

由于使用TiUP集群启动一直失败，故使用手动启动方式启动集群服务

具体顺序是：pd-2379 -> tikv-20160 -> tidb-4000 -> monitor-9100

最后在中控机启动：prometheus-9090、alertmanager-9093、grafana-3000

## 测试验证

#### 验证集群状态

```
#通过 TiUP 检查集群状态
tiup cluster display tidb-test
#登录数据库验证
mysql -u root -h 192.168.1.145 -P 4000
```

效果如下：

```
[tidb@localhost ~]$ tiup cluster display tidb-test
Starting component `cluster`: /home/tidb/.tiup/components/cluster/v1.3.1/tiup-cluster display tidb-test
Cluster type:       tidb
Cluster name:       tidb-test
Cluster version:    v4.0.0
SSH type:           builtin
Dashboard URL:      http://192.168.1.145:2379/dashboard
ID                   Role          Host           Ports        OS/Arch       Status    Data Dir                      Deploy Dir
--                   ----          ----           -----        -------       ------    --------                      ----------
192.168.1.120:9093   alertmanager  192.168.1.120  9093/9094    linux/x86_64  inactive  /tidb-data/alertmanager-9093  /tidb-deploy/alertmanager-9093
192.168.1.120:3000   grafana       192.168.1.120  3000         linux/x86_64  inactive  -                             /tidb-deploy/grafana-3000
192.168.1.120:2379   pd            192.168.1.120  2379/2380    linux/x86_64  Up|L      /tidb-data/pd-2379            /tidb-deploy/pd-2379
192.168.1.145:2379   pd            192.168.1.145  2379/2380    linux/x86_64  Up|UI     /tidb-data/pd-2379            /tidb-deploy/pd-2379
192.168.1.211:2379   pd            192.168.1.211  2379/2380    linux/x86_64  Up        /tidb-data/pd-2379            /tidb-deploy/pd-2379
192.168.1.120:9090   prometheus    192.168.1.120  9090         linux/x86_64  inactive  /tidb-data/prometheus-9090    /tidb-deploy/prometheus-9090
192.168.1.145:4000   tidb          192.168.1.145  4000/10080   linux/x86_64  Up        -                             /tidb-deploy/tidb-4000
192.168.1.211:4000   tidb          192.168.1.211  4000/10080   linux/x86_64  Up        -                             /tidb-deploy/tidb-4000
192.168.1.120:20160  tikv          192.168.1.120  20160/20180  linux/x86_64  Up        /tidb-data/tikv-20160         /tidb-deploy/tikv-20160
192.168.1.145:20160  tikv          192.168.1.145  20160/20180  linux/x86_64  Up        /tidb-data/tikv-20160         /tidb-deploy/tikv-20160
192.168.1.211:20160  tikv          192.168.1.211  20160/20180  linux/x86_64  Up        /tidb-data/tikv-20160         /tidb-deploy/tikv-20160
Total nodes: 11
```

```
[tidb@localhost ~]$ mysql -u root -h 192.168.1.145 -P 4000
Welcome to the MySQL monitor.  Commands end with ; or \g.
Your MySQL connection id is 16
Server version: 5.7.25-TiDB-v4.0.0 TiDB Server (Apache License 2.0) Community Edition, MySQL 5.7 compatible

Copyright (c) 2000, 2020, Oracle and/or its affiliates. All rights reserved.

Oracle is a registered trademark of Oracle Corporation and/or its
affiliates. Other names may be trademarks of their respective
owners.

Type 'help;' or '\h' for help. Type '\c' to clear the current input statement.

mysql> 
```



#### 验证监控

Prometheus

```
http://192.168.1.120:9090/graph
```

TiDB Dashboard

```
http://192.168.1.120:2379/dashboard/
```

Grafana

```
http://192.168.1.120:3000/
```

## 避坑点

1、ssh互信

2、手动启动pd-2379、tikv-20160、tidb-4000服务的脚本需要手动修改参数配置的地址，将0.0.0.0修改为当前机器的ip地址，如果使用docker安装可以不用修改

## 参阅资料

1、https://docs.pingcap.com/zh/tidb/stable/production-deployment-using-tiup 

2、https://www.cnblogs.com/dxxv/p/13411253.html