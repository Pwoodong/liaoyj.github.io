# TiDB集群搭建



## 系统机器准备

| 操作系统 | 版本 | IP            | 备注           |
| -------- | ---- | ------------- | -------------- |
| CentosOS | 8.2  | 192.168.1.120 | 中控机/PD/TiKV |
| CentosOS | 8.2  | 192.168.1.145 | PD/TiKV/TiDB   |
| CentosOS | 8.2  | 192.168.1.211 | PD/TiKV/TiDB   |

## 系统服务准备

### ntp服务

由于虚拟机版本8+已不支持ntp软件包，故使用wlnmp来安装ntp服务

```
#添加wlnmp源
rpm -ivh http://mirrors.wlnmp.com/centos/wlnmp-release-centos.noarch.rpm
#安装ntp服务
yum install wntp
#时间同步
ntpdate ntp1.aliyun.com
```

### ssh互信

以root用户登录，并创建tidb用户并设置密码

```
useradd tidb
passwd tidb
```

执行vimudo，配置sudo免密

```
visudo

#进入配置文件在最后添加
tidb ALL=(ALL) NOPASSWD: ALL
```

中控机以tidb登录，生成密钥

```
#执行ssh-keygen，一路回车
ssh-keygen
#进入.ssh目录下,拷贝公钥到服务机器
ssh-copy-id -i ~/.ssh/id_rsa.pub 192.168.1.145
ssh-copy-id -i ~/.ssh/id_rsa.pub 192.168.1.211
#中控机作为服务机器，因此也需要免密
cat id_rsa.pub >> authorized_keys
```

中控机测试免密

```
#ssh连接
ssh 192.168.1.120
ssh 192.168.1.145
ssh 192.168.1.211
#执行
sudo -su root
```

## 安装TiUP 

#### 在中控机上安装 TiUP 组件

```
#安装 TiUP 工具
curl --proto '=https' --tlsv1.2 -sSf https://tiup-mirrors.pingcap.com/install.sh | sh
#重新声明全局环境变量
source .bash_profile
#确认 TiUP 工具是否安装
which tiup
#安装 TiUP cluster 组件
tiup cluster
#如果已经安装，则更新 TiUP cluster 组件至最新版本
tiup update --self && tiup update cluster
#验证当前 TiUP cluster 版本信息
tiup --binary cluster
```

#### 初始化配置文件

```
global:
  user: "tidb"
  ssh_port: 22
  deploy_dir: "/tidb-deploy"
  data_dir: "/tidb-data"
monitored:
  node_exporter_port: 9100
  blackbox_exporter_port: 9115
server_configs:
  tidb:
    log.slow-threshold: 300
    binlog.enable: false
    binlog.ignore-error: false
  tikv:
    readpool.storage.use-unified-pool: false
    readpool.coprocessor.use-unified-pool: true
  pd:
    schedule.leader-schedule-limit: 4
    schedule.region-schedule-limit: 2048
    schedule.replica-schedule-limit: 64 
pd_servers:
  - host: 192.168.1.120
  - host: 192.168.1.145
  - host: 192.168.1.211
tidb_servers:
  - host: 192.168.1.145
  - host: 192.168.1.211
tikv_servers:
  - host: 192.168.1.120
  - host: 192.168.1.145
  - host: 192.168.1.211
monitoring_servers:
  - host: 192.168.1.120
grafana_servers:
  - host: 192.168.1.120
alertmanager_servers:
  - host: 192.168.1.120
```

#### 集群部署

```
#执行部署命令
tiup cluster deploy tidb-test v4.0.0 ./topology.yaml -y
#查看 TiUP 管理的集群情况
tiup cluster list
#检查部署的 TiDB 集群情况
tiup cluster display tidb-test
#启动集群
tiup cluster start tidb-test
```

由于使用TiUP集群启动一直失败，故使用手动启动方式启动集群服务

具体顺序是：pd-2379 -> tikv-20160 -> tidb-4000 -> monitor-9100

最后在中控机启动：prometheus-9090、alertmanager-9093、grafana-3000

## 测试验证

#### 验证集群状态

```
#通过 TiUP 检查集群状态
tiup cluster display tidb-test
#登录数据库验证
mysql -u root -h 192.168.1.145 -P 4000
```

#### 验证监控

Prometheus

```
http://192.168.1.120:9090/graph
```

TiDB Dashboard

```
http://192.168.1.120:2379/dashboard/
```

Grafana

```
http://192.168.1.120:3000/
```

## 避坑点

1、ssh互信

2、手动启动pd-2379、tikv-20160、tidb-4000服务的脚本需要手动修改参数配置的地址，将0.0.0.0修改为当前机器的ip地址，如果使用docker安装可以不用修改

### 参阅资料

1、https://docs.pingcap.com/zh/tidb/stable/production-deployment-using-tiup 

2、https://www.cnblogs.com/dxxv/p/13411253.html